<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Twister2 · High Performance Data Analytics</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="High Performance Data Analytics"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Twister2 · High Performance Data Analytics"/><meta property="og:type" content="website"/><meta property="og:url" content="https://twister2.org/"/><meta property="og:description" content="High Performance Data Analytics"/><meta property="og:image" content="https://twister2.org/img/undraw_online.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://twister2.org/img/undraw_tweetstorm.svg"/><link rel="shortcut icon" href="/img/favicon.ico"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-dark.min.css"/><link rel="alternate" type="application/atom+xml" href="https://twister2.org/blog/atom.xml" title="Twister2 Blog ATOM Feed"/><link rel="alternate" type="application/rss+xml" href="https://twister2.org/blog/feed.xml" title="Twister2 Blog RSS Feed"/><link rel="stylesheet" href="/css/code-blocks-buttons.css"/><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat|Oswald|Roboto&amp;display=swap"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script><script type="text/javascript" src="/js/code-blocks-buttons.js"></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/logo_large.png" alt="Twister2"/><h2 class="headerTitleWithLogo">Twister2</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/docs/introduction" target="_self">Getting Started</a></li><li class=""><a href="/docs/compiling/compile_overview" target="_self">Docs</a></li><li class=""><a href="/docs/examples/tset/hello_world" target="_self">Tutorial</a></li><li class=""><a href="/docs/examples/examples" target="_self">Examples</a></li><li class=""><a href="/docs/developers/debugging" target="_self">Contribute</a></li><li class=""><a href="/docs/download" target="_self">Download</a></li><li class="siteNavItemActive"><a href="/configs" target="_self">Configurations</a></li><li class=""><a href="https://github.com/DSC-SPIDAL/twister2" target="_blank">GitHub</a></li><li class=""><a href="/blog/" target="_self">Blog</a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="configs-wrapper"><h1>Twister2 Configurations</h1><h2>Common configurations</h2><h3>Checkpoint Configurations</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.checkpointing.enable</td><td>false</td><td>Enable or disable checkpointing</td></tr><tr><td>twister2.checkpointing.store</td><td>edu.iu.dsc.tws.checkpointing.stores.LocalFileStateStore</td><td>The implementation of the store to be used</td></tr><tr><td>twister2.checkpointing.store.fs.dir</td><td>${TWISTER2_HOME}/persistent/</td><td>Root directory of local file system based store</td></tr><tr><td>twister2.checkpointing.store.hdfs.dir</td><td>/twister2/persistent/</td><td>Root directory of hdfs based store</td></tr><tr><td>twister2.checkpointing.source.frequency</td><td>1000</td><td>Source triggering frequency</td></tr></tbody></table><h3>Data Configurations</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.data.hadoop.home</td><td>${HADOOP_HOME}</td><td></td></tr><tr><td>twister2.data.hdfs.url</td><td>hdfs://namenode:9000</td><td></td></tr><tr><td>twister2.data.hdfs.class</td><td>org.apache.hadoop.hdfs.DistributedFileSystem</td><td></td></tr><tr><td>twister2.data.hdfs.implementation.key</td><td>fs.hdfs.impl</td><td></td></tr><tr><td>twister2.data.hdfs.config.directory</td><td>${HADOOP_HOME}/etc/hadoop/core-site.xml</td><td></td></tr><tr><td>twister2.data.hdfs.data.directory</td><td>/user/username/</td><td></td></tr><tr><td>twister2.data.hdfs.namenode</td><td>namenode.domain.name</td><td></td></tr><tr><td>twister2.data.hdfs.namenode.port</td><td>9000</td><td></td></tr></tbody></table><h3>Network Configurations</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.network.buffer.size</td><td>1024000</td><td>the buffer size to be used</td></tr><tr><td>twister2.network.sendBuffer.count</td><td>4</td><td>number of send buffers to be used</td></tr><tr><td>twister2.network.receiveBuffer.count</td><td>4</td><td>number of receive buffers to be used</td></tr><tr><td>twister2.network.channel.pending.size</td><td>2048</td><td>channel pending messages</td></tr><tr><td>twister2.network.send.pending.max</td><td>4</td><td>the send pending messages</td></tr><tr><td>twister2.network.partition.message.group.low_water_mark</td><td>8000</td><td>group up to 8 ~ 16 messages</td></tr><tr><td>twister2.network.partition.message.group.high_water_mark</td><td>16000</td><td>this is the max number of messages to group</td></tr><tr><td>twister2.network.partition.batch.grouping.size</td><td>10000</td><td>in batch partition operations, this value will be used to create mini batches<br/>within partial receivers</td></tr><tr><td>twister2.network.ops.persistent.dirs</td><td>[&quot;${TWISTER2_HOME}/persistent/&quot;]</td><td>For disk based operations, this directory list will be used to persist incoming messages.<br/>This can be used to balance the load between multiple devices, by specifying directory locations<br/>from different devices.</td></tr><tr><td>twister2.network.shuffle.memory.bytes.max</td><td>102400000</td><td>the maximum amount of bytes kept in memory for operations that goes to disk</td></tr><tr><td>twister2.network.shuffle.memory.records.max</td><td>102400000</td><td>the maximum number of records kept in memory for operations that goes to dist</td></tr><tr><td>twister2.network.shuffle.file.bytes.max</td><td>10000000</td><td>size of the shuffle file (10MB default)</td></tr><tr><td>twister2.network.shuffle.parallel.io</td><td>2</td><td>no of parallel IO operations permitted</td></tr><tr><td>twister2.network.partition.algorithm.stream</td><td>simple</td><td>the partitioning algorithm</td></tr><tr><td>twister2.network.partition.algorithm.batch</td><td>simple</td><td>the partitioning algorithm</td></tr><tr><td>twister2.network.partition.algorithm.batch.keyed_gather</td><td>simple</td><td>the partitioning algorithm</td></tr><tr><td>ttwister2.network.partition.ring.group.workers</td><td>2</td><td>ring group worker</td></tr></tbody></table><h3>Resource Configurations</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.client.debug</td><td>&#x27;-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5006&#x27;</td><td>use this property to debug the client submitting the job</td></tr></tbody></table><h3>Core Configurations</h3><h3>Twister2 Job Master related settings</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.job.master.used</td><td>true</td><td></td></tr><tr><td>twister2.job.master.runs.in.client</td><td>true</td><td>if true, the job master runs in the submitting client<br/>if false, job master runs as a separate process in the cluster<br/>by default, it is true<br/>when the job master runs in the submitting client, this client has to be submitting the job from a machine in the cluster</td></tr><tr><td>twister2.job.master.assigns.worker.ids</td><td>false</td><td>if true, job master assigns the worker IDs,<br/>if false, workers have their IDs when regitering with the job master</td></tr><tr><td>twister2.worker.to.job.master.response.wait.duration</td><td>100000</td><td></td></tr></tbody></table><h3>WorkerController related config parameters</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.worker.controller.max.wait.time.for.all.workers.to.join</td><td>100000</td><td>amount of timeout for all workers to join the job<br/>in milli seconds</td></tr><tr><td>twister2.worker.controller.max.wait.time.on.barrier</td><td>100000</td><td>amount of timeout on barriers for all workers to arrive<br/>in milli seconds</td></tr></tbody></table><h3>Common thread pool config parameters</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.common.thread.pool.threads</td><td>2</td><td>Maximum number of threads to spawn on demand</td></tr><tr><td>twister2.common.thread.pool.keepalive</td><td>10</td><td>maximum time that excess idle threads will wait for new tasks before terminating</td></tr></tbody></table><h3>Dashboard related settings</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.dashboard.host</td><td>http://localhost:8080</td><td>Dashboard server host address and port<br/>if this parameter is not specified, then job master will not try to connect to Dashboard</td></tr></tbody></table><h3>Task Configurations</h3><h3>Task Scheduler Related Configurations</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.taskscheduler.streaming</td><td>roundrobin</td><td>Task scheduling mode for the streaming jobs &quot;roundrobin&quot; or &quot;firstfit&quot; or &quot;datalocalityaware&quot; or &quot;userdefined&quot;<br/>By default it is roundrobin mode.</td></tr><tr><td>twister2.taskscheduler.streaming.class</td><td>edu.iu.dsc.tws.tsched.streaming.roundrobin.RoundRobinTaskScheduler</td><td>Task Scheduler class for the round robin streaming task scheduler</td></tr><tr><td>twister2.taskscheduler.streaming.class</td><td>edu.iu.dsc.tws.tsched.streaming.datalocalityaware.DataLocalityStreamingTaskScheduler</td><td>Task Scheduler for the Data Locality Aware Streaming Task Scheduler</td></tr><tr><td>twister2.taskscheduler.streaming.class</td><td>edu.iu.dsc.tws.tsched.streaming.firstfit.FirstFitStreamingTaskScheduler</td><td>Task Scheduler for the FirstFit Streaming Task Scheduler</td></tr><tr><td>twister2.taskscheduler.streaming.class</td><td>edu.iu.dsc.tws.tsched.userdefined.UserDefinedTaskScheduler</td><td>Task Scheduler for the userDefined Streaming Task Scheduler</td></tr><tr><td>twister2.taskscheduler.batch</td><td>roundrobin</td><td>Task scheduling mode for the batch jobs &quot;roundrobin&quot; or &quot;datalocalityaware&quot; or &quot;userdefined&quot;<br/>By default it is roundrobin mode.</td></tr><tr><td>twister2.taskscheduler.batch.class</td><td>edu.iu.dsc.tws.tsched.batch.roundrobin.RoundRobinBatchTaskScheduler</td><td>Task Scheduler class for the round robin batch task scheduler</td></tr><tr><td>twister2.taskscheduler.batch.class</td><td>edu.iu.dsc.tws.tsched.batch.datalocalityaware.DataLocalityBatchTaskScheduler</td><td>Task Scheduler for the Data Locality Aware Batch Task Scheduler</td></tr><tr><td>twister2.taskscheduler.batch.class</td><td>edu.iu.dsc.tws.tsched.userdefined.UserDefinedTaskScheduler</td><td>Task Scheduler for the userDefined Batch Task Scheduler</td></tr><tr><td>twister2.taskscheduler.task.instances</td><td>2</td><td>Number of task instances to be allocated to each worker/container</td></tr><tr><td>twister2.taskscheduler.task.instance.ram</td><td>512.0</td><td>Ram value to be allocated to each task instance</td></tr><tr><td>twister2.taskscheduler.task.instance.disk</td><td>500.0</td><td>Disk value to be allocated to each task instance</td></tr><tr><td>twister2.taskscheduler.instance.cpu</td><td>2.0</td><td>CPU value to be allocated to each task instancetwister2.task.parallelism</td></tr><tr><td>twister2.taskscheduler.container.instance.ram</td><td>4096.0</td><td>Default Container Instance Values<br/>Ram value to be allocated to each container</td></tr><tr><td>twister2.taskscheduler.container.instance.disk</td><td>8000.0</td><td>Disk value to be allocated to each container</td></tr><tr><td>twister2.taskscheduler.container.instance.cpu</td><td>16.0</td><td></td></tr><tr><td>twister2.taskscheduler.ram.padding.container</td><td>2.0</td><td>Default Container Padding Values<br/>Default padding value of the ram to be allocated to each container</td></tr><tr><td>twister2.taskscheduler.disk.padding.container</td><td>12.0</td><td>Default padding value of the disk to be allocated to each container</td></tr><tr><td>twister2.taskscheduler.cpu.padding.container</td><td>1.0</td><td>CPU padding value to be allocated to each container</td></tr><tr><td>twister2.taskscheduler.container.padding.percentage</td><td>2</td><td>Percentage value to be allocated to each container</td></tr><tr><td>twister2.taskscheduler.container.instance.bandwidth</td><td>100 #Mbps</td><td>Static Default Network parameters<br/>Bandwidth value to be allocated to each container instance for datalocality scheduling</td></tr><tr><td>twister2.taskscheduler.container.instance.latency</td><td>0.002 #Milliseconds</td><td>Latency value to be allocated to each container instance for datalocality scheduling</td></tr><tr><td>twister2.taskscheduler.datanode.instance.bandwidth</td><td>200 #Mbps</td><td>Bandwidth to be allocated to each datanode instance for datalocality scheduling</td></tr><tr><td>twister2.taskscheduler.datanode.instance.latency</td><td>0.01 #Milliseconds</td><td>Latency value to be allocated to each datanode instance for datalocality scheduling</td></tr><tr><td>twister2.taskscheduler.task.parallelism</td><td>2</td><td>Prallelism value to each task instance</td></tr><tr><td>twister2.taskscheduler.task.type</td><td>streaming</td><td>Task type to each submitted job by default it is &quot;streaming&quot; job.</td></tr><tr><td>twister2.exector.worker.threads</td><td>1</td><td>number of threads per worker</td></tr><tr><td>twister2.executor.batch.name</td><td>edu.iu.dsc.tws.executor.threading.BatchSharingExecutor2</td><td>name of the batch executor</td></tr><tr><td>twister2.exector.instance.queue.low.watermark</td><td>10000</td><td>number of tuples executed at a single pass</td></tr></tbody></table><h2>Standalone configurations</h2><h3>Checkpoint Configurations</h3>No specific configurations<h3>Data Configurations</h3>No specific configurations<h3>Network Configurations</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.network.channel.class</td><td>edu.iu.dsc.tws.comms.mpi.TWSMPIChannel</td><td></td></tr></tbody></table><h3>Resource Configurations</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.resource.scheduler.mpi.working.directory</td><td>${HOME}/.twister2/jobs</td><td>working directory</td></tr><tr><td>twsiter2.resource.scheduler.mpi.mode</td><td>standalone</td><td>mode of the mpi scheduler</td></tr><tr><td>twister2.resource.scheduler.mpi.job.id</td><td></td><td>the job id file</td></tr><tr><td>twister2.resource.scheduler.mpi.shell.script</td><td>mpi.sh</td><td>slurm script to run</td></tr><tr><td>twister2.resource.scheduler.mpi.home</td><td></td><td>the mpirun command location</td></tr><tr><td>twister2.system.package.uri</td><td>${TWISTER2_DIST}/twister2-core-0.4.0.tar.gz</td><td>the package uri</td></tr><tr><td>twister2.class.launcher</td><td>edu.iu.dsc.tws.rsched.schedulers.standalone.MPILauncher</td><td>the launcher class</td></tr><tr><td>twister2.resource.scheduler.mpi.mpirun.file</td><td>twister2-core/ompi/bin/mpirun</td><td>mpi run file, this assumes a mpirun that is shipped with the product<br/>change this to just mpirun if you are using a system wide installation of OpenMPI<br/>or complete path of OpenMPI in case you have something custom</td></tr><tr><td>twister2.resource.scheduler.mpi.mapby</td><td>node</td><td>mpi scheduling policy. Two possible options are node and slot.<br/>read more at https://www.open-mpi.org/faq/?category=running#mpirun-scheduling</td></tr><tr><td>twister2.resource.scheduler.mpi.mapby.use-pe</td><td>false</td><td>use mpi map-by modifier PE. If this option is enabled, cpu count of compute resource<br/>specified in job definition will be taken into consideration</td></tr><tr><td>twister2.resource.sharedfs</td><td>true</td><td>Indicates whether bootstrap process needs to be run and distribute job file and core<br/>between MPI nodes. Twister2 assumes job file is accessible to all nodes if this property is set<br/>to true, else it will run the bootstrap process</td></tr><tr><td>twister2.resource.fs.mount</td><td>${TWISTER2_HOME}/persistent/fs/</td><td>Directory for file system volume mount</td></tr><tr><td>twister2.uploader.directory</td><td>${HOME}/.twister2/repository</td><td>the uploader directory</td></tr><tr><td>twister2.class.uploader</td><td>edu.iu.dsc.tws.rsched.uploaders.localfs.LocalFileSystemUploader</td><td>the uplaoder class</td></tr><tr><td>twister2.uploader.download.method</td><td>HTTP</td><td>this is the method that workers use to download the core and job packages<br/>it could be  HTTP, HDFS, ..</td></tr></tbody></table><h3>Core Configurations</h3><h3>Twister2 Job Master related settings</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.job.master.used</td><td>true</td><td></td></tr><tr><td>twister2.job.master.runs.in.client</td><td>true</td><td>if true, the job master runs in the submitting client<br/>if false, job master runs as a separate process in the cluster<br/>by default, it is true<br/>when the job master runs in the submitting client, this client has to be submitting the job from a machine in the cluster</td></tr><tr><td>twister2.job.master.assigns.worker.ids</td><td>false</td><td>if true, job master assigns the worker IDs,<br/>if false, workers have their IDs when regitering with the job master</td></tr><tr><td>twister2.worker.to.job.master.response.wait.duration</td><td>100000</td><td></td></tr></tbody></table><h3>WorkerController related config parameters</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.worker.controller.max.wait.time.for.all.workers.to.join</td><td>100000</td><td>amount of timeout for all workers to join the job<br/>in milli seconds</td></tr><tr><td>twister2.worker.controller.max.wait.time.on.barrier</td><td>100000</td><td>amount of timeout on barriers for all workers to arrive<br/>in milli seconds</td></tr></tbody></table><h3>Common thread pool config parameters</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.common.thread.pool.threads</td><td>2</td><td>Maximum number of threads to spawn on demand</td></tr><tr><td>twister2.common.thread.pool.keepalive</td><td>10</td><td>maximum time that excess idle threads will wait for new tasks before terminating</td></tr></tbody></table><h3>Dashboard related settings</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.dashboard.host</td><td>http://localhost:8080</td><td>Dashboard server host address and port<br/>if this parameter is not specified, then job master will not try to connect to Dashboard</td></tr></tbody></table><h3>Task Configurations</h3>No specific configurations<h2>Slurm configurations</h2><h3>Checkpoint Configurations</h3>No specific configurations<h3>Data Configurations</h3>No specific configurations<h3>Network Configurations</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.network.channel.class</td><td>edu.iu.dsc.tws.comms.mpi.TWSMPIChannel</td><td></td></tr></tbody></table><h3>Resource Configurations</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.resource.scheduler.mpi.working.directory</td><td>${HOME}/.twister2/jobs</td><td>working directory</td></tr><tr><td>twsiter2.resource.scheduler.mpi.mode</td><td>slurm</td><td>mode of the mpi scheduler</td></tr><tr><td>twister2.resource.scheduler.mpi.job.id</td><td></td><td>the job id file</td></tr><tr><td>twister2.resource.scheduler.mpi.shell.script</td><td>mpi.sh</td><td>slurm script to run</td></tr><tr><td>twister2.resource.scheduler.slurm.partition</td><td>juliet</td><td>slurm partition</td></tr><tr><td>twister2.resource.scheduler.mpi.home</td><td></td><td>the mpirun command location</td></tr><tr><td>twister2.system.package.uri</td><td>${TWISTER2_DIST}/twister2-core-0.4.0.tar.gz</td><td>the package uri</td></tr><tr><td>twister2.class.launcher</td><td>edu.iu.dsc.tws.rsched.schedulers.standalone.MPILauncher</td><td>the launcher class</td></tr><tr><td>twister2.resource.scheduler.mpi.mpirun.file</td><td>twister2-core/ompi/bin/mpirun</td><td>mpi run file, this assumes a mpirun that is shipped with the product<br/>change this to just mpirun if you are using a system wide installation of OpenMPI<br/>or complete path of OpenMPI in case you have something custom</td></tr><tr><td>twister2.uploader.directory</td><td>${HOME}/.twister2/repository</td><td>the uploader directory</td></tr><tr><td>twister2.class.uploader</td><td>edu.iu.dsc.tws.rsched.uploaders.localfs.LocalFileSystemUploader</td><td>the uplaoder class</td></tr></tbody></table><h3>Core Configurations</h3><h3>WorkerController related config parameters</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.worker.controller.max.wait.time.for.all.workers.to.join</td><td>100000</td><td>amount of timeout for all workers to join the job<br/>in milli seconds</td></tr><tr><td>twister2.worker.controller.max.wait.time.on.barrier</td><td>100000</td><td>amount of timeout on barriers for all workers to arrive<br/>in milli seconds</td></tr></tbody></table><h3>Dashboard related settings</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.dashboard.host</td><td>http://localhost:8080</td><td>Dashboard server host address and port<br/>if this parameter is not specified, then job master will not try to connect to Dashboard</td></tr></tbody></table><h3>Task Configurations</h3>No specific configurations<h2>Aurora configurations</h2><h3>Checkpoint Configurations</h3>No specific configurations<h3>Data Configurations</h3>No specific configurations<h3>Network Configurations</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.network.channel.class</td><td>edu.iu.dsc.tws.comms.tcp.TWSTCPChannel</td><td></td></tr></tbody></table><h3>Resource Configurations</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.system.package.uri</td><td>${TWISTER2_DIST}/twister2-core-0.4.0.tar.gz</td><td>the package uri</td></tr><tr><td>twister2.class.launcher</td><td>edu.iu.dsc.tws.rsched.schedulers.aurora.AuroraLauncher</td><td>launcher class for aurora submission</td></tr><tr><td>twister2.class.uploader</td><td>edu.iu.dsc.tws.rsched.uploaders.scp.ScpUploader</td><td>the uploader class<hr/><b>Options</b><ul><li>edu.iu.dsc.tws.rsched.uploaders.NullUploader</li><li>edu.iu.dsc.tws.rsched.uploaders.localfs.LocalFileSystemUploader</li></ul></td></tr><tr><td>twister2.job.worker.class</td><td>edu.iu.dsc.tws.examples.internal.rsched.BasicAuroraContainer</td><td>container class to run in workers</td></tr><tr><td>twister2.class.aurora.worker</td><td>edu.iu.dsc.tws.rsched.schedulers.aurora.AuroraWorkerStarter</td><td>the Aurora worker class</td></tr></tbody></table><h3>ZooKeeper related config parameters</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.zookeeper.server.addresses</td><td>149.165.150.81:2181</td><td><hr/><b>Options</b><ul><li>127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002</li></ul></td></tr><tr><td>#twister2.zookeeper.root.node.path</td><td>/twister2</td><td>the root node path of this job on ZooKeeper<br/>the default is &quot;/twister2&quot;</td></tr><tr><td>twister2.zookeeper.max.wait.time.for.all.workers.to.join</td><td>100000</td><td>if the workers want to wait for all others to join a job, max wait time in ms</td></tr></tbody></table><h3>Uploader configuration</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.uploader.directory</td><td>/root/.twister2/repository/</td><td>the directory where the file will be uploaded, make sure the user has the necessary permissions<br/>to upload the file here.</td></tr><tr><td>twister2.uploader.directory.repository</td><td>/root/.twister2/repository/</td><td></td></tr><tr><td>twister2.uploader.scp.command.options</td><td></td><td>This is the scp command options that will be used by the uploader, this can be used to<br/>specify custom options such as the location of ssh keys.</td></tr><tr><td>twister2.uploader.scp.command.connection</td><td>root@149.165.150.81</td><td>The scp connection string sets the remote user name and host used by the uploader.</td></tr><tr><td>twister2.uploader.ssh.command.options</td><td></td><td>The ssh command options that will be used when connecting to the uploading host to execute<br/>command such as delete files, make directories.</td></tr><tr><td>twister2.uploader.ssh.command.connection</td><td>root@149.165.150.81</td><td>The ssh connection string sets the remote user name and host used by the uploader.</td></tr></tbody></table><h3>Client configuration parameters for submission of twister2 jobs</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.resource.scheduler.aurora.script</td><td>${TWISTER2_CONF}/twister2.aurora</td><td>aurora python script to submit a job to Aurora Scheduler<br/>its default value is defined as the following in the code<br/>can be reset from this config file if desired</td></tr><tr><td>twister2.resource.scheduler.aurora.cluster</td><td>example</td><td>cluster name aurora scheduler runs in</td></tr><tr><td>twister2.resource.scheduler.aurora.role</td><td>www-data</td><td>role in cluster</td></tr><tr><td>twister2.resource.scheduler.aurora.env</td><td>devel</td><td>environment name</td></tr><tr><td>twister2.job.name</td><td>basic-aurora</td><td>aurora job name<hr/><b>Options</b><ul><li>basic-aurora</li></ul></td></tr><tr><td>twister2.worker.cpu</td><td>1.0</td><td>number of cores for each worker<br/>it is a floating point number<br/>each worker can have fractional cores such as 0.5 cores or multiple cores as 2<br/>default value is 1.0 core</td></tr><tr><td>twister2.worker.ram</td><td>200</td><td>amount of memory for each worker in the job in mega bytes as integer<br/>default value is 200 MB</td></tr><tr><td>twister2.worker.disk</td><td>1024</td><td>amount of hard disk space on each worker in mega bytes<br/>this only used when running twister2 in Aurora<br/>default value is 1024 MB.</td></tr><tr><td>twister2.worker.instances</td><td>6</td><td>number of worker instances</td></tr></tbody></table><h3>Core Configurations</h3>No specific configurations<h3>Task Configurations</h3>No specific configurations<h2>Kubernetes configurations</h2><h3>Checkpoint Configurations</h3>No specific configurations<h3>Data Configurations</h3>No specific configurations<h3>Network Configurations</h3><h3>OpenMPI settings</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.network.channel.class</td><td>edu.iu.dsc.tws.comms.tcp.TWSTCPChannel</td><td>If the channel is set as TWSMPIChannel,<br/>the job is started as OpenMPI job<br/>Otherwise, it is a regular twister2 job. OpenMPI is not started in this case.</td></tr><tr><td>kubernetes.secret.name</td><td>twister2-openmpi-ssh-key</td><td>A Secret object must be present in Kubernetes master<br/>Its name must be specified here</td></tr></tbody></table><h3>Worker port settings</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>kubernetes.worker.base.port</td><td>9000</td><td>the base port number workers will use internally to communicate with each other<br/>when there are multiple workers in a pod, first worker will get this port number,<br/>second worker will get the next port, and so on.<br/>default value is 9000,</td></tr><tr><td>kubernetes.worker.transport.protocol</td><td>TCP</td><td>transport protocol for the worker. TCP or UDP<br/>by default, it is TCP<br/>set if it is UDP</td></tr></tbody></table><h3>NodePort service parameters</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>kubernetes.node.port.service.requested</td><td>true</td><td>if the job requests NodePort service, it must be true<br/>NodePort service makes the workers accessible from external entities (outside of the cluster)<br/>by default, its value is false</td></tr><tr><td>kubernetes.service.node.port</td><td>30003</td><td>if NodePort value is 0, it is automatically assigned a value<br/>the user can request a specific port value in the NodePort range by setting the value below<br/>by default Kubernetes uses the range 30000-32767 for NodePorts<br/>Kubernetes admins can change this range</td></tr></tbody></table><h3>Resource Configurations</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.docker.image.for.kubernetes</td><td>twister2/twister2-k8s:0.4.0</td><td>Twister2 Docker image for Kubernetes<hr/><b>Options</b><ul><li>twister2/twister2-k8s:0.3.0-au</li></ul></td></tr><tr><td>twister2.system.package.uri</td><td>${TWISTER2_DIST}/twister2-core-0.3.0.tar.gz</td><td>the package uri</td></tr><tr><td>twister2.class.launcher</td><td>edu.iu.dsc.tws.rsched.schedulers.k8s.KubernetesLauncher</td><td></td></tr><tr><td>kubernetes.image.pull.policy</td><td>Always</td><td>image pull policy, by default is IfNotPresent<br/>it could also be Always</td></tr></tbody></table><h3>Kubernetes Mapping and Binding parameters</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>kubernetes.bind.worker.to.cpu</td><td>true</td><td>Statically bind workers to CPUs<br/>Workers do not move from the CPU they are started during computation<br/>twister2.cpu_per_container has to be an integer<br/>by default, its value is false</td></tr><tr><td>kubernetes.worker.to.node.mapping</td><td>true</td><td>kubernetes can map workers to nodes as specified by the user<br/>default value is false</td></tr><tr><td>kubernetes.worker.mapping.key</td><td>kubernetes.io/hostname</td><td>the label key on the nodes that will be used to map workers to nodes</td></tr><tr><td>kubernetes.worker.mapping.operator</td><td>In</td><td>operator to use when mapping workers to nodes based on key value<br/>Exists/DoesNotExist checks only the existence of the specified key in the node.<br/>Ref https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity-beta-feature</td></tr><tr><td>kubernetes.worker.mapping.values</td><td>[&#x27;e012&#x27;, &#x27;e013&#x27;]</td><td>values for the mapping key<br/>when the mapping operator is either Exists or DoesNotExist, values list must be empty.<hr/><b>Options</b><ul><li>[]</li></ul></td></tr><tr><td>Valid values</td><td>all-same-node, all-separate-nodes, none</td><td>uniform worker mapping<br/>default value is none<hr/><b>Options</b><ul><li>all-same-node</li></ul></td></tr></tbody></table><h3>ZooKeeper related config parameters</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.zookeeper.server.addresses</td><td>ip:port</td><td><hr/><b>Options</b><ul><li>127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002</li></ul></td></tr><tr><td>twister2.zookeeper.root.node.path</td><td>/twister2</td><td>the root node path of this job on ZooKeeper<br/>the default is &quot;/twister2&quot;</td></tr></tbody></table><h3>When a job is submitted, the job package needs to be transferred to worker pods<br/>Two upload methods are provided:<br/>  a) Job Package Transfer Using kubectl file copy (default)<br/>  b) Job Package Transfer Through a Web Server<br/>Following two configuration parameters control the uploading with the first method<br/>when the submitting client uploads the job package directly to pods using kubectl copy</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.kubernetes.client.to.pods.uploading</td><td>true</td><td>if the value of this parameter is true,<br/>the job package is transferred from submitting client to pods directly<br/>if it is false, the job package will be transferred to pods through the upload web server<br/>default value is true</td></tr><tr><td>twister2.kubernetes.uploader.watch.pods.starting</td><td>true</td><td>When the job package is transferred from submitting client to pods directly,<br/>upload attempts can either start after watching pods or immediately when StatefulSets are created<br/>watching pods before starting file upload attempts is more accurate<br/>it may be slightly slower to transfer the job package by watching pods though<br/>default value is true</td></tr></tbody></table><h3>Following configuration parameters sets up the upload web server,<br/>when the job package is transferred through a webserver<br/>Workers download the job package from this web server</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.uploader.directory</td><td>/absolute/path/to/uploder/directory/</td><td>the directory where the job package file will be uploaded,<br/>make sure the user has the necessary permissions to upload the file there.<br/>full directory path to upload the job package with scp</td></tr><tr><td>twister2.uploader.directory.repository</td><td>/absolute/path/to/uploder/directory/</td><td></td></tr><tr><td>twister2.download.directory</td><td>http://webserver.address:port/download/dir/path</td><td>web server link of the job package download directory<br/>this is the same directory as the uploader directory</td></tr><tr><td>twister2.uploader.scp.command.options</td><td>--chmod=+rwx</td><td>This is the scp command options that will be used by the uploader, this can be used to<br/>specify custom options such as the location of ssh keys.</td></tr><tr><td>twister2.uploader.scp.command.connection</td><td>user@uploadserver.address</td><td>The scp connection string sets the remote user name and host used by the uploader.</td></tr><tr><td>twister2.uploader.ssh.command.options</td><td></td><td>The ssh command options that will be used when connecting to the uploading host to execute<br/>command such as delete files, make directories.</td></tr><tr><td>twister2.uploader.ssh.command.connection</td><td>user@uploadserver.address</td><td>The ssh connection string sets the remote user name and host used by the uploader.</td></tr><tr><td>twister2.class.uploader</td><td>edu.iu.dsc.tws.rsched.uploaders.scp.ScpUploader</td><td>the uploader class<hr/><b>Options</b><ul><li>edu.iu.dsc.tws.rsched.uploaders.NullUploader</li><li>edu.iu.dsc.tws.rsched.uploaders.localfs.LocalFileSystemUploader</li></ul></td></tr><tr><td>twister2.uploader.download.method</td><td>HTTP</td><td>this is the method that workers use to download the core and job packages<br/>it could be  HTTP, HDFS, ..</td></tr></tbody></table><h3>Job configuration parameters for submission of twister2 jobs</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.job.name</td><td>t2-job</td><td>twister2 job name</td></tr><tr><td>    # number of workers using this compute resource</td><td>instances * workersPerPod</td><td>A Twister2 job can have multiple sets of compute resources<br/>instances shows the number of compute resources to be started with this specification<br/>workersPerPod shows the number of workers on each pod in Kubernetes.<br/>   May be omitted in other clusters. default value is 1.</td></tr><tr><td>#- cpu</td><td>0.5  # number of cores for each worker, may be fractional such as 0.5 or 2.4</td><td><hr/><b>Options</b><ul><li>1024 # ram for each worker as Mega bytes</li><li>1.0 # volatile disk for each worker as Giga bytes</li><li>2 # number of compute resource instances with this specification</li><li>false # only one ComputeResource can be scalable in a job</li><li>1 # number of workers on each pod in Kubernetes. May be omitted in other clusters.</li></ul></td></tr><tr><td>twister2.job.driver.class</td><td>edu.iu.dsc.tws.examples.internal.rsched.DriverExample</td><td>driver class to run</td></tr><tr><td>twister2.job.worker.class</td><td>edu.iu.dsc.tws.examples.internal.rsched.BasicK8sWorker</td><td>worker class to run<hr/><b>Options</b><ul><li>edu.iu.dsc.tws.examples.internal.comms.BroadcastCommunication</li><li>edu.iu.dsc.tws.examples.batch.sort.SortJob</li><li>edu.iu.dsc.tws.examples.internal.BasicNetworkTest</li><li>edu.iu.dsc.tws.examples.comms.batch.BReduceExample</li><li>edu.iu.dsc.tws.examples.internal.BasicNetworkTest</li><li>edu.iu.dsc.tws.examples.batch.comms.batch.BReduceExample</li></ul></td></tr><tr><td>twister2.worker.additional.ports</td><td>[&quot;port1&quot;, &quot;port2&quot;, &quot;port3&quot;]</td><td>by default each worker has one port<br/>additional ports can be requested for all workers in a job<br/>please provide the requested port names as a list</td></tr></tbody></table><h3>Kubernetes related settings<br/>namespace to use in kubernetes<br/>default value is &quot;default&quot;</h3><h3>Node locations related settings<br/>If this parameter is set as true,<br/>Twister2 will use the below lists for node locations:<br/>  kubernetes.datacenters.list<br/>  kubernetes.racks.list<br/>Otherwise, it will try to get these information by querying Kubernetes Master<br/>It will use below two labels when querying node locations<br/>For this to work, submitting client has to have admin privileges</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>rack.labey.key</td><td>rack</td><td>rack label key for Kubernetes nodes in a cluster<br/>each rack should have a unique label<br/>all nodes in a rack should share this label<br/>Twister2 workers can be scheduled by using these label values<br/>Better data locality can be achieved<br/>no default value is specified</td></tr><tr><td>datacenter.labey.key</td><td>datacenter</td><td>data center label key<br/>each data center should have a unique label<br/>all nodes in a data center should share this label<br/>Twister2 workers can be scheduled by using these label values<br/>Better data locality can be achieved<br/>no default value is specified</td></tr><tr><td>  - echo</td><td>[&#x27;blue-rack&#x27;, &#x27;green-rack&#x27;]</td><td>Data center list with rack names</td></tr><tr><td>  - green-rack</td><td>[&#x27;node11.ip&#x27;, &#x27;node12.ip&#x27;, &#x27;node13.ip&#x27;]</td><td>Rack list with node IPs in them</td></tr></tbody></table><h3>persistent volume related settings</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>persistent.volume.per.worker</td><td>0.0</td><td>persistent volume size per worker in GB as double<br/>default value is 0.0Gi<br/>set this value to zero, if you have not persistent disk support<br/>when this value is zero, twister2 will not try to set up persistent storage for this job</td></tr><tr><td>kubernetes.persistent.storage.class</td><td>twister2-nfs-storage</td><td>the admin should provide a PersistentVolume object with the following storage class.<br/>Default storage class name is &quot;twister2&quot;.</td></tr><tr><td>kubernetes.storage.access.mode</td><td>ReadWriteMany</td><td>persistent storage access mode.<br/>It shows the access mode for workers to access the shared persistent storage.<br/>if it is &quot;ReadWriteMany&quot;, many workers can read and write<br/>https://kubernetes.io/docs/concepts/storage/persistent-volumes</td></tr></tbody></table><h3>Core Configurations</h3><h3>Logging related settings for Twister2 workers</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.logging.level</td><td>INFO</td><td>default value is INFO</td></tr><tr><td>persistent.logging.requested</td><td>true</td><td>Do workers request persistent logging? it could be true or false<br/>default value is false</td></tr><tr><td>twister2.logging.redirect.sysouterr</td><td>false</td><td>whether System.out and System.err should be redircted to log files<br/>When System.out and System.err are redirected to log file, <br/>All messages are only saved in log files. Only a few intial messages are shown on Dashboard.<br/>Otherwise, Dashboard has the complete messages,<br/>log files has the log messages except System.out and System.err.</td></tr><tr><td>twister2.logging.max.file.size.mb</td><td>100</td><td>The maximum log file size in MB</td></tr><tr><td>twister2.logging.maximum.files</td><td>5</td><td>The maximum number of log files for each worker</td></tr></tbody></table><h3>Twister2 Job Master related settings</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.job.master.runs.in.client</td><td>false</td><td>if true, the job master runs in the submitting client<br/>if false, job master runs as a separate process in the cluster <br/>by default, it is true<br/>when the job master runs in the submitting client,<br/>this client has to be submitting the job from a machine in the cluster<br/>getLocalHost must return a reachable IP address to the job master</td></tr><tr><td>twister2.job.master.assigns.worker.ids</td><td>false</td><td>if true, job master assigns the worker IDs,<br/>if false, workers have their IDs when registering with the job master<br/>it is always false in Kubernetes<br/>setting it to true does not make any difference</td></tr><tr><td>twister2.worker.ping.interval</td><td>10000</td><td>ping message intervals from workers to the job master in milliseconds<br/>default value is 10seconds = 10000</td></tr><tr><td>twister2.job.master.port</td><td>11011</td><td>twister2 job master port number<br/>default value is 11011</td></tr><tr><td>twister2.worker.to.job.master.response.wait.duration</td><td>10000</td><td>worker to job master response wait time in milliseconds<br/>this is for messages that wait for a response from the job master<br/>default value is 10seconds = 10000</td></tr><tr><td>twister2.job.master.volatile.volume.size</td><td>0.0</td><td>twister2 job master volatile volume size in GB<br/>default value is 1.0 Gi<br/>if this value is 0, volatile volume is not setup for job master</td></tr><tr><td>twister2.job.master.persistent.volume.size</td><td>0.0</td><td>twister2 job master persistent volume size in GB<br/>default value is 1.0 Gi<br/>if this value is 0, persistent volume is not setup for job master</td></tr><tr><td>twister2.job.master.cpu</td><td>0.2</td><td>twister2 job master cpu request<br/>default value is 0.2 percentage</td></tr><tr><td>twister2.job.master.ram</td><td>1024</td><td>twister2 job master RAM request in MB<br/>default value is 1024 MB</td></tr></tbody></table><h3>WorkerController related config parameters</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.worker.controller.max.wait.time.for.all.workers.to.join</td><td>100000</td><td>amount of timeout for all workers to join the job<br/>in milli seconds</td></tr><tr><td>twister2.worker.controller.max.wait.time.on.barrier</td><td>100000</td><td>amount of timeout on barriers for all workers to arrive<br/>in milli seconds</td></tr></tbody></table><h3>Dashboard related settings</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.dashboard.host</td><td>http://twister2-dashboard.default.svc.cluster.local</td><td>Dashboard server host address and port<br/>if this parameter is not specified, then job master will not try to connect to Dashboard<br/>if dashboard is running as a statefulset in the cluster</td></tr></tbody></table><h3>Task Configurations</h3>No specific configurations<h2>Mesos configurations</h2><h3>Checkpoint Configurations</h3>No specific configurations<h3>Data Configurations</h3>No specific configurations<h3>Network Configurations</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.network.channel.class</td><td>edu.iu.dsc.tws.comms.tcp.TWSTCPChannel</td><td></td></tr></tbody></table><h3>Resource Configurations</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.scheduler.mesos.scheduler.working.directory</td><td>~/.twister2/repository&quot;#&quot;${TWISTER2_DIST}/topologies/${CLUSTER}/${ROLE}/${TOPOLOGY}</td><td>working directory for the topologies</td></tr><tr><td>twister2.directory.core-package</td><td>/root/.twister2/repository/twister2-core/</td><td></td></tr><tr><td>twister2.directory.sandbox.java.home</td><td>${JAVA_HOME}</td><td>location of java - pick it up from shell environment</td></tr><tr><td>twister2.mesos.master.uri</td><td>149.165.150.81:5050</td><td>The URI of Mesos Master</td></tr><tr><td>twister2.mesos.framework.name</td><td>Twister2 framework</td><td>mesos framework name</td></tr><tr><td>twister2.mesos.master.uri</td><td>zk://localhost:2181/mesos</td><td></td></tr><tr><td>twister2.mesos.framework.staging.timeout.ms</td><td>2000</td><td>The maximum time in milliseconds waiting for MesosFramework got registered with Mesos Master</td></tr><tr><td>twister2.mesos.scheduler.driver.stop.timeout.ms</td><td>5000</td><td>The maximum time in milliseconds waiting for Mesos Scheduler Driver to complete stop()</td></tr><tr><td>twister2.mesos.native.library.path</td><td>/usr/lib/mesos/0.28.1/lib/</td><td>the path to load native mesos library</td></tr><tr><td>twister2.system.package.uri</td><td>${TWISTER2_DIST}/twister2-core-0.4.0.tar.gz</td><td>the core package uri</td></tr><tr><td>twister2.mesos.overlay.network.name</td><td>mesos-overlay</td><td></td></tr><tr><td>twister2.docker.image.name</td><td>gurhangunduz/twister2-mesos:docker-mpi</td><td></td></tr><tr><td>twister2.system.job.uri</td><td>http://localhost:8082/twister2/mesos/twister2-job.tar.gz</td><td>the job package uri for mesos agent to fetch.<br/>For fetching http server must be running on mesos master</td></tr><tr><td>twister2.class.launcher</td><td>edu.iu.dsc.tws.rsched.schedulers.mesos.MesosLauncher</td><td>launcher class for mesos submission</td></tr><tr><td>twister2.job.worker.class</td><td>edu.iu.dsc.tws.examples.internal.comms.BroadcastCommunication</td><td>container class to run in workers</td></tr><tr><td>twister2.class.mesos.worker</td><td>edu.iu.dsc.tws.rsched.schedulers.mesos.MesosWorker</td><td>the Mesos worker class</td></tr></tbody></table><h3>ZooKeeper related config parameters</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.zookeeper.server.addresses</td><td>localhost:2181</td><td><hr/><b>Options</b><ul><li>127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002</li></ul></td></tr><tr><td>#twister2.zookeeper.root.node.path</td><td>/twister2</td><td>the root node path of this job on ZooKeeper<br/>the default is &quot;/twister2&quot;</td></tr><tr><td>twister2.uploader.directory</td><td>/var/www/html/twister2/mesos/</td><td>the directory where the file will be uploaded, make sure the user has the necessary permissions<br/>to upload the file here.</td></tr><tr><td>#twister2.uploader.directory.repository</td><td>/var/www/html/twister2/mesos/</td><td></td></tr><tr><td>twister2.uploader.scp.command.options</td><td>--chmod=+rwx</td><td>This is the scp command options that will be used by the uploader, this can be used to<br/>specify custom options such as the location of ssh keys.</td></tr><tr><td>twister2.uploader.scp.command.connection</td><td>root@149.165.150.81</td><td>The scp connection string sets the remote user name and host used by the uploader.</td></tr><tr><td>twister2.uploader.ssh.command.options</td><td></td><td>The ssh command options that will be used when connecting to the uploading host to execute<br/>command such as delete files, make directories.</td></tr><tr><td>twister2.uploader.ssh.command.connection</td><td>root@149.165.150.81</td><td>The ssh connection string sets the remote user name and host used by the uploader.</td></tr><tr><td>twister2.class.uploader</td><td>edu.iu.dsc.tws.rsched.uploaders.scp.ScpUploader</td><td>the uploader class<hr/><b>Options</b><ul><li>edu.iu.dsc.tws.rsched.uploaders.NullUploader</li><li>edu.iu.dsc.tws.rsched.uploaders.localfs.LocalFileSystemUploader</li></ul></td></tr><tr><td>twister2.uploader.download.method</td><td>HTTP</td><td>this is the method that workers use to download the core and job packages<br/>it could be  HTTP, HDFS, ..</td></tr><tr><td>twister2.HTTP.fetch.uri</td><td>http://149.165.150.81:8082</td><td>HTTP fetch uri</td></tr></tbody></table><h3>Client configuration parameters for submission of twister2 jobs</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.resource.scheduler.mesos.cluster</td><td>example</td><td>cluster name mesos scheduler runs in</td></tr><tr><td>twister2.resource.scheduler.mesos.role</td><td>www-data</td><td>role in cluster</td></tr><tr><td>twister2.resource.scheduler.mesos.env</td><td>devel</td><td>environment name</td></tr><tr><td>twister2.job.name</td><td>basic-mesos</td><td>mesos job name</td></tr><tr><td>  #  workersPerPod</td><td>2 # number of workers on each pod in Kubernetes. May be omitted in other clusters.</td><td>A Twister2 job can have multiple sets of compute resources<br/>instances shows the number of compute resources to be started with this specification<br/>workersPerPod shows the number of workers on each pod in Kubernetes.<br/>   May be omitted in other clusters. default value is 1.</td></tr><tr><td>    instances</td><td>4 # number of compute resource instances with this specification</td><td><hr/><b>Options</b><ul><li>2 # number of workers on each pod in Kubernetes. May be omitted in other clusters.</li></ul></td></tr><tr><td>twister2.worker.additional.ports</td><td>[&quot;port1&quot;, &quot;port2&quot;, &quot;port3&quot;]</td><td>by default each worker has one port<br/>additional ports can be requested for all workers in a job<br/>please provide the requested port names as a list</td></tr><tr><td>twister2.job.driver.class</td><td>edu.iu.dsc.tws.examples.internal.rsched.DriverExample</td><td>driver class to run</td></tr><tr><td>nfs.server.address</td><td>149.165.150.81</td><td>nfs server address</td></tr><tr><td>nfs.server.path</td><td>/nfs/shared-mesos/twister2</td><td>nfs server path</td></tr><tr><td>twister2.worker_port</td><td>31000</td><td>worker port</td></tr><tr><td>twister2.desired_nodes</td><td>all</td><td>desired nodes</td></tr><tr><td>twister2.use_docker_container</td><td>true</td><td></td></tr><tr><td>rack.labey.key</td><td>rack</td><td>rack label key for Mesos nodes in a cluster<br/>each rack should have a unique label<br/>all nodes in a rack should share this label<br/>Twister2 workers can be scheduled by using these label values<br/>Better data locality can be achieved<br/>no default value is specified</td></tr><tr><td>datacenter.labey.key</td><td>datacenter</td><td>data center label key<br/>each data center should have a unique label<br/>all nodes in a data center should share this label<br/>Twister2 workers can be scheduled by using these label values<br/>Better data locality can be achieved<br/>no default value is specified</td></tr><tr><td>  - echo</td><td>[&#x27;blue-rack&#x27;, &#x27;green-rack&#x27;]</td><td>Data center list with rack names</td></tr><tr><td>  - blue-rack</td><td>[&#x27;10.0.0.40&#x27;, &#x27;10.0.0.41&#x27;, &#x27;10.0.0.42&#x27;, &#x27;10.0.0.43&#x27;, &#x27;10.0.0.44&#x27;, ]</td><td>Rack list with node IPs in them</td></tr></tbody></table><h3>Core Configurations</h3><h3>Logging related settings for Twister2 workers</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.logging.level</td><td>INFO</td><td>default value is INFO</td></tr><tr><td>persistent.logging.requested</td><td>true</td><td>Do workers request persistent logging? it could be true or false<br/>default value is false</td></tr><tr><td>twister2.logging.redirect.sysouterr</td><td>true</td><td>whether System.out and System.err should be redircted to log files<br/>When System.out and System.err are redirected to log file,<br/>All messages are only saved in log files. Only a few intial messages are shown on Dashboard.<br/>Otherwise, Dashboard has the complete messages,<br/>log files has the log messages except System.out and System.err.</td></tr><tr><td>twister2.logging.max.file.size.mb</td><td>100</td><td>The maximum log file size in MB</td></tr><tr><td>twister2.logging.maximum.files</td><td>5</td><td>The maximum number of log files for each worker</td></tr></tbody></table><h3>Twister2 Job Master related settings</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.job.master.runs.in.client</td><td>false</td><td>if true, the job master runs in the submitting client<br/>if false, job master runs as a separate process in the cluster<br/>by default, it is true<br/>when the job master runs in the submitting client, this client has to be submitting the job from a machine in the cluster</td></tr><tr><td>twister2.job.master.assigns.worker.ids</td><td>false</td><td>if true, job master assigns the worker IDs,<br/>if false, workers have their IDs when regitering with the job master</td></tr><tr><td>twister2.worker.ping.interval</td><td>10000</td><td>ping message intervals from workers to the job master in milliseconds<br/>default value is 10seconds = 10000</td></tr><tr><td>#twister2.job.master.port</td><td>2023</td><td>twister2 job master port number<br/>default value is 11111</td></tr><tr><td>twister2.worker.to.job.master.response.wait.duration</td><td>10000</td><td>worker to job master response wait time in milliseconds<br/>this is for messages that wait for a response from the job master<br/>default value is 10seconds = 10000</td></tr><tr><td>twister2.job.master.volatile.volume.size</td><td>1.0</td><td>twister2 job master volatile volume size in GB<br/>default value is 1.0 Gi<br/>if this value is 0, volatile volume is not setup for job master</td></tr><tr><td>twister2.job.master.persistent.volume.size</td><td>1.0</td><td>twister2 job master persistent volume size in GB<br/>default value is 1.0 Gi<br/>if this value is 0, persistent volume is not setup for job master</td></tr><tr><td>twister2.job.master.cpu</td><td>0.2</td><td>twister2 job master cpu request<br/>default value is 0.2 percentage</td></tr><tr><td>twister2.job.master.ram</td><td>1000</td><td>twister2 job master RAM request in MB<br/>default value is 0.2 percentage</td></tr><tr><td>twister2.job.master.ip</td><td>149.165.150.81</td><td></td></tr></tbody></table><h3>WorkerController related config parameters</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.worker.controller.max.wait.time.for.all.workers.to.join</td><td>100000</td><td>amount of timeout for all workers to join the job<br/>in milli seconds</td></tr><tr><td>twister2.worker.controller.max.wait.time.on.barrier</td><td>100000</td><td>amount of timeout on barriers for all workers to arrive<br/>in milli seconds</td></tr></tbody></table><h3>Dashboard related settings</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.dashboard.host</td><td>http://localhost:8080</td><td>Dashboard server host address and port<br/>if this parameter is not specified, then job master will not try to connect to Dashboard</td></tr></tbody></table><h3>Task Configurations</h3>No specific configurations<h2>Nomad configurations</h2><h3>Checkpoint Configurations</h3>No specific configurations<h3>Data Configurations</h3>No specific configurations<h3>Network Configurations</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.network.channel.class</td><td>edu.iu.dsc.tws.comms.tcp.TWSTCPChannel</td><td></td></tr></tbody></table><h3>Resource Configurations</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.resource.scheduler.mpi.working.directory</td><td>${HOME}/.twister2/jobs</td><td>working directory</td></tr><tr><td>twister2.job.package.url</td><td>http://localhost:8082/twister2/mesos/twister2-job.tar.gz</td><td></td></tr><tr><td>twister2.core.package.url</td><td>http://localhost:8082/twister2/mesos/twister2-core-0.2.2.tar.gz</td><td></td></tr><tr><td>twister2.class.launcher</td><td>edu.iu.dsc.tws.rsched.schedulers.nomad.NomadLauncher</td><td>the launcher class</td></tr><tr><td>#twister2.nomad.scheduler.uri</td><td>http://149.165.150.81:4646</td><td>The URI of Nomad API</td></tr><tr><td>twister2.nomad.scheduler.uri</td><td>http://127.0.0.1:4646</td><td></td></tr><tr><td>twister2.nomad.core.freq.mapping</td><td>2000</td><td>The nomad schedules cpu resources in terms of clock frequency (e.g. MHz), while Heron topologies<br/>specify cpu requests in term of cores.  This config maps core to clock freqency.</td></tr><tr><td>twister2.filesystem.shared</td><td>true</td><td>weather we are in a shared file system, if that is the case, each worker will not download the<br/>core package and job package, otherwise they will download those packages</td></tr><tr><td>twister2.nomad.shell.script</td><td>nomad.sh</td><td>name of the script</td></tr><tr><td>twister2.system.package.uri</td><td>${TWISTER2_DIST}/twister2-core-0.4.0.tar.gz</td><td>path to the system core package</td></tr><tr><td>twister2.zookeeper.server.addresses</td><td>localhost:2181</td><td><hr/><b>Options</b><ul><li>127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002</li></ul></td></tr><tr><td>twister2.uploader.directory</td><td>/tmp/repository</td><td>the directory where the file will be uploaded, make sure the user has the necessary permissions<br/>to upload the file here.<br/>if you want to run it on a local machine use this value</td></tr><tr><td>#twister2.uploader.directory</td><td>/var/www/html/twister2/mesos/</td><td>if you want to use http server on echo</td></tr><tr><td>twister2.uploader.directory.repository</td><td>/var/www/html/twister2/mesos/</td><td></td></tr><tr><td>twister2.uploader.scp.command.options</td><td>--chmod=+rwx</td><td>This is the scp command options that will be used by the uploader, this can be used to<br/>specify custom options such as the location of ssh keys.</td></tr><tr><td>twister2.uploader.scp.command.connection</td><td>root@localhost</td><td>The scp connection string sets the remote user name and host used by the uploader.</td></tr><tr><td>twister2.uploader.ssh.command.options</td><td></td><td>The ssh command options that will be used when connecting to the uploading host to execute<br/>command such as delete files, make directories.</td></tr><tr><td>twister2.uploader.ssh.command.connection</td><td>root@localhost</td><td>The ssh connection string sets the remote user name and host used by the uploader.</td></tr><tr><td>twister2.class.uploader</td><td>edu.iu.dsc.tws.rsched.uploaders.localfs.LocalFileSystemUploader</td><td>file system uploader to be used<hr/><b>Options</b><ul><li>edu.iu.dsc.tws.rsched.uploaders.scp.ScpUploader</li></ul></td></tr><tr><td>twister2.uploader.download.method</td><td>LOCAL</td><td>this is the method that workers use to download the core and job packages<br/>it could be  LOCAL,  HTTP, HDFS, ..</td></tr></tbody></table><h3>client related configurations for job submit</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>nfs.server.address</td><td>localhost</td><td>nfs server address</td></tr><tr><td>nfs.server.path</td><td>/nfs/shared/twister2</td><td>nfs server path</td></tr><tr><td>rack.labey.key</td><td>rack</td><td>rack label key for Mesos nodes in a cluster<br/>each rack should have a unique label<br/>all nodes in a rack should share this label<br/>Twister2 workers can be scheduled by using these label values<br/>Better data locality can be achieved<br/>no default value is specified</td></tr><tr><td>datacenter.labey.key</td><td>datacenter</td><td>data center label key<br/>each data center should have a unique label<br/>all nodes in a data center should share this label<br/>Twister2 workers can be scheduled by using these label values<br/>Better data locality can be achieved<br/>no default value is specified</td></tr><tr><td>  - echo</td><td>[&#x27;blue-rack&#x27;, &#x27;green-rack&#x27;]</td><td>Data center list with rack names</td></tr><tr><td>  - green-rack</td><td>[&#x27;node11.ip&#x27;, &#x27;node12.ip&#x27;, &#x27;node13.ip&#x27;]</td><td>Rack list with node IPs in them</td></tr><tr><td>  #  workersPerPod</td><td>2 # number of workers on each pod in Kubernetes. May be omitted in other clusters.</td><td>A Twister2 job can have multiple sets of compute resources<br/>instances shows the number of compute resources to be started with this specification<br/>workersPerPod shows the number of workers on each pod in Kubernetes.<br/>   May be omitted in other clusters. default value is 1.</td></tr><tr><td>    instances</td><td>4 # number of compute resource instances with this specification</td><td><hr/><b>Options</b><ul><li>2 # number of workers on each pod in Kubernetes. May be omitted in other clusters.</li></ul></td></tr><tr><td>twister2.worker.additional.ports</td><td>[&quot;port1&quot;, &quot;port2&quot;, &quot;port3&quot;]</td><td>by default each worker has one port<br/>additional ports can be requested for all workers in a job<br/>please provide the requested port names as a list</td></tr><tr><td>twister2.worker_port</td><td>31000</td><td>worker port</td></tr></tbody></table><h3>Core Configurations</h3><h3>Logging related settings<br/>for Twister2 workers</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.logging.level</td><td>INFO</td><td>logging level, FINEST, FINER, FINE, CONFIG, INFO, WARNING, SEVERE</td></tr><tr><td>persistent.logging.requested</td><td>true</td><td>Do workers request persistent logging? it could be true or false<br/>default value is false</td></tr><tr><td>twister2.logging.redirect.sysouterr</td><td>true</td><td>whether System.out and System.err should be redirected to log files<br/>When System.out and System.err are redirected to log file,<br/>All messages are only saved in log files. Only a few initial messages are shown on Dashboard.<br/>Otherwise, Dashboard has the complete messages,<br/>log files has the log messages except System.out and System.err.</td></tr><tr><td>twister2.logging.max.file.size.mb</td><td>100</td><td>The maximum log file size in MB</td></tr><tr><td>twister2.logging.maximum.files</td><td>5</td><td>The maximum number of log files for each worker</td></tr><tr><td>twister2.logging.sandbox.logging</td><td>true</td><td></td></tr></tbody></table><h3>Twister2 Job Master related settings</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.job.master.runs.in.client</td><td>true</td><td>if true, the job master runs in the submitting client<br/>if false, job master runs as a separate process in the cluster<br/>by default, it is true<br/>when the job master runs in the submitting client, this client has to be submitting the job from a machine in the cluster</td></tr><tr><td>twister2.job.master.assigns.worker.ids</td><td>false</td><td>if true, job master assigns the worker IDs,<br/>if false, workers have their IDs when registering with the job master</td></tr><tr><td>twister2.worker.ping.interval</td><td>10000</td><td>ping message intervals from workers to the job master in milliseconds<br/>default value is 10seconds = 10000</td></tr><tr><td>twister2.job.master.port</td><td>11011</td><td>twister2 job master port number<br/>default value is 11111</td></tr><tr><td>twister2.worker.to.job.master.response.wait.duration</td><td>10000</td><td>worker to job master response wait time in milliseconds<br/>this is for messages that wait for a response from the job master<br/>default value is 10seconds = 10000</td></tr><tr><td>twister2.job.master.volatile.volume.size</td><td>1.0</td><td>twister2 job master volatile volume size in GB<br/>default value is 1.0 Gi<br/>if this value is 0, volatile volume is not setup for job master</td></tr><tr><td>twister2.job.master.persistent.volume.size</td><td>1.0</td><td>twister2 job master persistent volume size in GB<br/>default value is 1.0 Gi<br/>if this value is 0, persistent volume is not setup for job master</td></tr><tr><td>twister2.job.master.cpu</td><td>0.2</td><td>twister2 job master cpu request<br/>default value is 0.2 percentage</td></tr><tr><td>twister2.job.master.ram</td><td>1000</td><td>twister2 job master RAM request in MB<br/>default value is 0.2 percentage</td></tr><tr><td>twister2.job.master.ip</td><td>localhost</td><td>the job master ip to be used, this is used only in client based masters</td></tr></tbody></table><h3>WorkerController related config parameters</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.worker.controller.max.wait.time.for.all.workers.to.join</td><td>1000000</td><td>amount of timeout for all workers to join the job<br/>in milli seconds</td></tr><tr><td>twister2.worker.controller.max.wait.time.on.barrier</td><td>1000000</td><td>amount of timeout on barriers for all workers to arrive<br/>in milli seconds</td></tr></tbody></table><h3>Dashboard related settings</h3><table><thead><tr><td width="25%">Name</td><td width="25%">Default</td><td width="50%">Description</td></tr></thead><tbody><tr><td>twister2.dashboard.host</td><td>http://localhost:8080</td><td>Dashboard server host address and port<br/>if this parameter is not specified, then job master will not try to connect to Dashboard</td></tr></tbody></table><h3>Task Configurations</h3>No specific configurations</div><footer class="nav-footer" id="footer"><section class="sitemap"><a href="/" class="nav-home"><img src="/img/logo_large.png" alt="Twister2" width="66" height="58"/></a><div><h5>Docs</h5><a href="/docs/en/quickstart.html">Getting Started (Quickstart)</a><a href="/docs/en/concepts/api_overview">Guides (Programming Guides)</a></div><div><h5>Community</h5><a href="https://stackoverflow.com/questions/tagged/" target="_blank" rel="noreferrer noopener">Stack Overflow</a><a href="https://dsc-twister.slack.com/">Project Chat</a></div><div><h5>More</h5><a href="/blog">Blog</a><a href="https://github.com/DSC-SPIDAL/twister2">GitHub</a><a class="github-button" data-icon="octicon-star" data-count-href="/facebook/docusaurus/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star</a></div></section><section class="copyright">Copyright © 2019 Indiana University</section></footer></div></body></html>