################################################################################
# Client configuration parameters for submission of twister2 jobs
################################################################################
# A Twister2 job can have multiple sets of compute resources
# Four fields are mandatory: cpu, ram, disk and instances
# instances shows the number of compute resources to be started with this specification
# workersPerPod shows the number of workers on each pod in Kubernetes.
#    May be omitted in other clusters. default value is 1.
worker.compute.resources:
- cpu: 1  # number of cores for each worker, may be fractional such as 0.5 or 2.4
  ram: 1024 # ram for each worker as Mega bytes
  disk: 1.0 # volatile disk for each worker as Giga bytes
  instances: 6 # number of compute resource instances with this specification
#  workersPerPod: 2 # number of workers on each pod in Kubernetes. May be omitted in other clusters.

- cpu: 2  # number of cores for each worker, may be fractional such as 0.5 or 2.4
  ram: 1024 # ram for each worker as mega bytes
  disk: 1.0 # volatile disk for each worker as giga bytes. May be zero.
  instances: 4 # number of compute resource instances with this specification
#  workersPerPod: 2 # number of workers on each pod in Kubernetes. May be omitted in other clusters.

# by default each worker has one port
# additional ports can be requested for all workers in a job
# please provide the requested port names as a list
twister2.worker.additional.ports: ["port1", "port2", "port3"]

# driver class to run
twister2.job.driver.class: "edu.iu.dsc.tws.examples.internal.rsched.DriverExample"

# nfs server address
nfs.server.address: "149.165.150.81"

# nfs server path
nfs.server.path: "/nfs/shared-mesos/twister2"

# worker port
twister2.worker_port: "31000"

# desired nodes
#twister2.desired_nodes: "149.165.150.xx,149.165.150.xx,149.165.150.xx"
twister2.desired_nodes: "all"

twister2.use_docker_container: "true"

# rack label key for Mesos nodes in a cluster
# each rack should have a unique label
# all nodes in a rack should share this label
# Twister2 workers can be scheduled by using these label values
# Better data locality can be achieved
# Example: rack=rack1, rack=rack2, rack=rack3, etc
# no default value is specified
rack.labey.key: rack

# data center label key
# each data center should have a unique label
# all nodes in a data center should share this label
# Twister2 workers can be scheduled by using these label values
# Better data locality can be achieved
# Example: datacenter=dc1, datacenter=dc1, datacenter=dc1, etc
# no default value is specified
datacenter.labey.key: datacenter

# Data center list with rack names
datacenters.list:
- echo: ['blue-rack', 'green-rack']

# Rack list with node IPs in them
racks.list:
- blue-rack: ['10.0.0.40', '10.0.0.41', '10.0.0.42', '10.0.0.43', '10.0.0.44', ]
- green-rack: ['node11.ip', 'node12.ip', 'node13.ip']